{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# TP3: Spark et Parquet\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Antoine Cheneau, Elie Caratgé"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration initiale"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "books.csv size: 787.65 MB\n"
                    ]
                }
            ],
            "source": [
                "import re\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.rdd import PipelinedRDD\n",
                "import os\n",
                "\n",
                "regex = re.compile(\"[^a-zA-Z ]\")\n",
                "\n",
                "\n",
                "def remove_non_letters(word):\n",
                "    return regex.sub(\"\", word)\n",
                "\n",
                "\n",
                "def get_spark_session_and_words_rdd() -> tuple[SparkSession, PipelinedRDD]:\n",
                "    # Réinitialisation de la session Spark\n",
                "    spark = SparkSession.builder.getOrCreate()\n",
                "    spark.stop()\n",
                "    spark = SparkSession.builder.getOrCreate()\n",
                "\n",
                "    # Lecture du fichier books.csv et prétraitement\n",
                "    books_rdd: PipelinedRDD = spark.read.csv(\n",
                "        \"books.csv\", header=True, inferSchema=True, sep=\";\"\n",
                "    ).rdd\n",
                "    books_rdd = books_rdd.map(lambda row: remove_non_letters(row[1]))\n",
                "    books_rdd = books_rdd.flatMap(lambda line: line.split(\" \"))\n",
                "    books_rdd = books_rdd.filter(lambda word: len(word) > 0)\n",
                "    return spark, books_rdd\n",
                "\n",
                "\n",
                "# Print taille du fichier csv en MB\n",
                "file_size = os.path.getsize(\"books.csv\") / (1024 * 1024)\n",
                "print(f\"books.csv size: {file_size:.2f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercice 1 : Interface Spark UI\n",
                "\n",
                "Après avoir lancé Spark, nous pouvons accéder à l'interface Spark UI à l'adresse http://localhost:4040.\n",
                "\n",
                "Dans l'interface Spark UI standard, on trouve normalement les onglets suivants :\n",
                "- Jobs\n",
                "- Stages\n",
                "- Storage\n",
                "- Environment\n",
                "- Executors\n",
                "- SQL / DataFrame\n",
                "\n",
                "Cependant, dans notre configuration locale, certains onglets ne sont pas disponibles car ils dépendent des fonctionnalités que nous utilisons. Les onglets qui manquent sont :\n",
                "- Structured Streaming\n",
                "- Streaming (DStreams)\n",
                "- JDBC/ODBC Server"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercice 2 : Analyse des exécuteurs\n",
                "\n",
                "Dans l'onglet \"Executors\" du Spark UI, on peut voir le nombre de coeurs utilisés par chaque exécuteur. Il y a un unique exécuteur local qui utilise tous les coeurs disponibles, c'est-à-dire 12 coeurs. Cet exécuteur pourra lancer jusqu'à 12 tâches en parallèle.\n",
                "\n",
                "Pour utiliser cette information pour mieux configurer et exécuter notre application Spark :\n",
                "\n",
                "1. **Parallélisme optimal** : Nous pouvons ajuster le niveau de parallélisme de notre application en fonction du nombre de coeurs disponibles. Par exemple, si nous avons 12 coeurs, nous pourrions configurer `spark.default.parallelism` à un multiple de 12 (comme 24 ou 36) pour maximiser l'utilisation des ressources.\n",
                "\n",
                "2. **Partition des données** : Nous pouvons également ajuster le nombre de partitions de nos RDD en fonction du nombre de cœurs. Une bonne pratique est d'avoir 2-3 partitions par coeur disponible.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercice 3 : Algorithme naïf pour compter les mots de longueur paire/impaire\n",
                "\n",
                "Implémentons un algorithme \"naïf\" pour compter combien de mots ont un nombre pair ou impair de caractères dans le fichier books.csv."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Even length words: 58705920\n",
                        "Odd length words: 72253440\n"
                    ]
                }
            ],
            "source": [
                "spark, words_rdd = get_spark_session_and_words_rdd()\n",
                "\n",
                "# Algorithme naïf : utilise deux actions de comptage distinctes\n",
                "odd_words = words_rdd.filter(lambda word: len(word) % 2 != 0).count()\n",
                "even_words = words_rdd.filter(lambda word: len(word) % 2 == 0).count()\n",
                "\n",
                "print(f\"Even length words: {even_words}\")\n",
                "print(f\"Odd length words: {odd_words}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Liste des jobs\n",
                "\n",
                "![Exercice 3 - Jobs](../imgs/ex-3-jobs.png)\n",
                "\n",
                "### Détails d'un job \"count\"\n",
                "![Exercice 3 - Job detail](../imgs/ex-3-job-detail.png)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Analyse du DAG et de l'accès aux données\n",
                "\n",
                "Dans l'algorithme naïf ci-dessus, deux opérations d'action distinctes (`count`) sont exécutées sur des RDD filtrés.\n",
                "\n",
                "**Analyse du DAG:**\n",
                "- Le DAG montre deux jobs distincts, un pour chaque opération `count`\n",
                "- Le fichier csv est décomposé en 12 partitions, permettant un traitement parallèle\n",
                "\n",
                "\n",
                "\n",
                "**Accès aux données:**\n",
                "- Chacun des jobs lit le fichier csv en entier, ce qui entraîne une double lecture des données\n",
                "- Le processus complet de traitement est répété pour chaque job\n",
                "\n",
                "Cette implémentation n'est pas optimale car elle entraîne une double lecture des données et un traitement redondant."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercice 4 : Algorithme MapReduce pour compter les mots de longueur paire/impaire\n",
                "\n",
                "Implémentons maintenant un algorithme de type MapReduce pour effectuer le même comptage, mais de manière plus efficace."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Even length words: 58705920\n",
                        "Odd length words: 72253440\n"
                    ]
                }
            ],
            "source": [
                "def word_length_mapper(word):\n",
                "    return (len(word) % 2, 1)\n",
                "\n",
                "\n",
                "def word_length_reducer(a, b):\n",
                "    return a + b\n",
                "\n",
                "\n",
                "spark, books_rdd = get_spark_session_and_words_rdd()\n",
                "\n",
                "# Perform MapReduce\n",
                "word_length_counts = (\n",
                "    books_rdd.map(word_length_mapper).reduceByKey(word_length_reducer)\n",
                ").collectAsMap()\n",
                "\n",
                "# Affichage des résultats\n",
                "print(\"Even length words:\", word_length_counts.get(0, 0))\n",
                "print(\"Odd length words:\", word_length_counts.get(1, 0))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Liste des jobs\n",
                "![Exercice 4 - Jobs](../imgs/ex-4-jobs.png)\n",
                "\n",
                "### Détails du job de calcul\n",
                "![Exercice 4 - Job detail](../imgs/ex-4-job-detail.png)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Analyse du DAG et de l'accès aux données\n",
                "\n",
                "Dans cette implémentation MapReduce, nous utilisons une approche plus efficace avec un seul job Spark.\n",
                "\n",
                "**Analyse du DAG:**\n",
                "- Le DAG montre deux stages comprenant les étapes suivantes :\n",
                "  1. Lecture et transformation des données en tuples avec comme clé 0 ou 1 pour pair ou impair.\n",
                "  2. Un second stage causé par l'opération `reduceByKey`, qui regroupe les données par clé (0 pour pair, 1 pour impair) et effectue le comptage.\n",
                "\n",
                "**Accès aux données:**\n",
                "- Les données sont lues **une seule fois** depuis le fichier source\n",
                "- Un seul parcours des données de csv est nécessaire pour obtenir à la fois le nombre de mots de longueur paire et impaire\n",
                "- L'algorithme effectue une opération de shuffle qui envoie les données aux bons réducteurs selon leur clé (0 pour pair, 1 pour impair)\n",
                "- Cette approche est beaucoup plus efficace pour les grands ensembles de données, car l'accès disque est minimisé\n",
                "\n",
                "Cette implémentation MapReduce est plus efficace car elle ne nécessite qu'un seul parcours des données, ce qui réduit considérablement le temps d'exécution et les ressources utilisées, en particulier pour les jeux de données volumineux."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercice 5 : Comparaison des deux approches\n",
                "\n",
                "Comparons maintenant les deux approches en termes d'accès aux données:\n",
                "\n",
                "**Approche naïve (Exercice 3):**\n",
                "- Nécessite deux jobs Spark distincts\n",
                "- Les données sont lues et traitées deux fois\n",
                "- Chaque job effectue un filtrage différent (pair ou impair)\n",
                "- Plus d'opérations de lecture et plus de calculs redondants\n",
                "\n",
                "**Approche MapReduce (Exercice 4):**\n",
                "- Un seul job Spark de calcul\n",
                "- Les données sont lues et transformées une seule fois\n",
                "- La classification des mots (pair/impair) se fait en une seule passe\n",
                "\n",
                "\n",
                "Pour améliorer davantage les performances, nous pourrions ajuster les paramètres de configuration Spark, comme :\n",
                "- `spark.default.parallelism` : Niveau de parallélisme par défaut\n",
                "- `spark.sql.shuffle.partitions` : Nombre de partitions pour les opérations de shuffle\n",
                "- `spark.executor.memory` : Mémoire allouée à chaque exécuteur\n",
                "- `spark.driver.memory` : Mémoire allouée au pilote\n",
                "- `spark.memory.fraction` : Fraction de mémoire utilisée pour l'exécution et le stockage"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercice 6 : Optimisation de la configuration Spark\n",
                "\n",
                "En analysant l'onglet Executors et Environment du Spark UI, on observe qu'un exécuteur utilisant les 12 coeurs de la machine est présent. Chaque partition sera traitée par cet unique exécuteur, avec un maximum de 12 tâches en parallèle. Afin de pouvoir traiter plus de partitions en parallèle, nous pouvons essayer d'utiliser 4 exécuteurs avec 3 coeurs chacun et observer l'impact sur les performances."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercice 7 : Comptage de l'occurrence des mots"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Top 10 word counts:\n",
                        "the: 9031680\n",
                        "and: 5644800\n",
                        "of: 5143040\n",
                        "a: 3637760\n",
                        "i: 2383360\n",
                        "to: 2257920\n",
                        "was: 2132480\n",
                        "in: 2132480\n",
                        "that: 2007040\n",
                        "by: 1505280\n"
                    ]
                }
            ],
            "source": [
                "spark, words_rdd = get_spark_session_and_words_rdd()\n",
                "\n",
                "# Transformation des mots en minuscules pour avoir un comptage cohérent\n",
                "words_rdd = words_rdd.map(lambda word: word.lower())\n",
                "\n",
                "# Compter les occurrences de chaque mot\n",
                "word_counts = (\n",
                "    words_rdd.map(lambda word: (word, 1))\n",
                "    .reduceByKey(lambda a, b: a + b)\n",
                "    .sortBy(lambda x: x[1], ascending=False)\n",
                "    .collect()\n",
                ")\n",
                "\n",
                "# Afficher les 10 premiers résultats\n",
                "print(\"Top 10 word counts:\")\n",
                "for word, count in word_counts[:10]:\n",
                "    print(f\"{word}: {count}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Liste des jobs\n",
                "![Exercice 7 - Jobs](../imgs/ex-7-jobs.png)\n",
                "\n",
                "### Détails du job de calcul\n",
                "![Exercice 7 - Job detail](../imgs/ex-7-job-detail.png)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Analyse du DAG et de l'interface UI\n",
                "\n",
                "En examinant le DAG pour cette opération, nous pouvons dire que celui ci est similaire à celui de l'exercice 4 avec un shuffle de données causé par le `reduceByKey`. Un job de type `sortBy` a été ajouté pour trier les mots par ordre alphabétique avant de les collecter.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercice 8 : Recherche du mot le plus utilisé\n",
                "\n",
                "### Partie 1 : Utilisation de la fonction de tri de Spark"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Le mot le plus utilisé (méthode Spark) est 'the' avec 9031680 occurrences\n"
                    ]
                }
            ],
            "source": [
                "spark, words_rdd = get_spark_session_and_words_rdd()\n",
                "words_rdd = words_rdd.map(lambda word: word.lower())\n",
                "\n",
                "# Méthode 1: Utilisation de la fonction de tri intégrée de Spark\n",
                "word_counts = (\n",
                "    words_rdd.map(lambda word: (word, 1))\n",
                "    .reduceByKey(lambda a, b: a + b)\n",
                "    .sortBy(lambda x: x[1], ascending=False)\n",
                ")\n",
                "\n",
                "# Récupérer le mot le plus fréquent\n",
                "most_common_word = word_counts.first()\n",
                "print(\n",
                "    f\"Le mot le plus utilisé (méthode Spark) est '{most_common_word[0]}' avec {most_common_word[1]} occurrences\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Partie 2 : Implémentation d'une fonction de tri personnalisée"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Le mot le plus utilisé (méthode personnalisée) est 'the' avec 9031680 occurrences\n"
                    ]
                }
            ],
            "source": [
                "from operator import add\n",
                "import random\n",
                "\n",
                "\n",
                "def quick_sort(data):\n",
                "    \"\"\"\n",
                "    Implémentation du QuickSort pour trier une liste de tuples (mot, fréquence)\n",
                "    par ordre décroissant de fréquence\n",
                "    \"\"\"\n",
                "    if len(data) <= 1:\n",
                "        return data\n",
                "\n",
                "    pivot = data[random.randint(0, len(data) - 1)]\n",
                "    pivot_value = pivot[1]\n",
                "\n",
                "    greater = [x for x in data if x[1] > pivot_value]\n",
                "    equal = [x for x in data if x[1] == pivot_value]\n",
                "    lesser = [x for x in data if x[1] < pivot_value]\n",
                "\n",
                "    return quick_sort(greater) + equal + quick_sort(lesser)\n",
                "\n",
                "\n",
                "def sort_partition(iterator):\n",
                "    \"\"\"\n",
                "    Trie les éléments d'une partition en utilisant QuickSort\n",
                "    \"\"\"\n",
                "    partition_data = list(iterator)\n",
                "    if not partition_data:\n",
                "        return []\n",
                "\n",
                "    return quick_sort(partition_data)\n",
                "\n",
                "\n",
                "# Code principal\n",
                "spark, words_rdd = get_spark_session_and_words_rdd()\n",
                "words_rdd = words_rdd.map(lambda word: word.lower())\n",
                "\n",
                "\n",
                "# Implémentation manuelle avec QuickSort\n",
                "word_counts_manual = (\n",
                "    words_rdd.map(lambda word: (word, 1))\n",
                "    .reduceByKey(add)  # Utiliser add de operator est plus efficace\n",
                "    .coalesce(1)  # Rassembler toutes les données sur une seule partition\n",
                "    .mapPartitions(sort_partition)  # Appliquer QuickSort sur la partition\n",
                ")\n",
                "\n",
                "most_common_word_manual = word_counts_manual.first()\n",
                "print(\n",
                "    f\"Le mot le plus utilisé (méthode personnalisée) est '{most_common_word_manual[0]}' avec {most_common_word_manual[1]} occurrences\"\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Analyse comparative des deux approches\n",
                "\n",
                "On observe que les deux approches sont similaires en termes de structure de DAG et d'accès aux données, mais il y a quelques différences notables :\n",
                "   - La première approche permet de calculer le résultat en 8s vs 12s pour la seconde approche.\n",
                "   - La seconde étape rajoute un stage \"coalesce\" qui n'est pas nécessaire dans la première approche.\n",
                "   - Rajouter l'opération coalesce empêche la parallélisation de l'opération de tri, ce qui entraîne une baisse de performance."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercice 9\n",
                "\n",
                "Lorsqu'on considère le stockage de documents homogènes dans un environnement distribué, le format Parquet offre des avantages significatifs pour l'accès aux champs spécifiques :\n",
                "\n",
                "1. **Organisation columnaire :** Parquet stocke les données par colonnes plutôt que par lignes. Cette structure permet d'accéder directement aux champs spécifiques sans avoir à lire l'ensemble du document, réduisant considérablement la quantité de données à traiter et à transférer sur le réseau.\n",
                "\n",
                "2. **Compression :** Parquet compresse chaque colonne indépendamment avec des algorithmes adaptés à son type de données. Les valeurs similaires regroupées ensemble se compressent mieux, ce qui réduit l'espace de stockage et accélère les transferts de données lors de l'accès aux champs spécifiques.\n",
                "\n",
                "3. **Statistiques :** Parquet stocke des statistiques (min/max, comptages) pour chaque bloc de données. Ces métadonnées permettent d'implémenter des techniques d'optimisation comme le \"predicate pushdown\", qui élimine les blocs non pertinents avant même de les lire, accélérant considérablement les opérations de filtrage sur des champs particuliers."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exercice 10"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Résultat avec Spark :\n",
                        "+----------+------+\n",
                        "|   country| count|\n",
                        "+----------+------+\n",
                        "| \"\"Italy\"\"|224761|\n",
                        "+----------+------+\n",
                        "only showing top 1 row\n",
                        "\n",
                        "Résultat avec Pandas :\n",
                        "      country   count\n",
                        "0   \"\"Italy\"\"  224761\n"
                    ]
                }
            ],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.functions import col, count, explode, split, trim\n",
                "\n",
                "spark = SparkSession.builder.getOrCreate()\n",
                "df_parquet = spark.read.parquet(\"restaurants.parquet\")\n",
                "\n",
                "# Parquet\n",
                "country_count_spark = df_parquet.groupBy(\"country\").count().orderBy(col(\"count\").desc())\n",
                "print(\"Résultat avec Spark :\")\n",
                "country_count_spark.show(1)\n",
                "\n",
                "# Pandas\n",
                "country_count_pandas = country_count_spark.toPandas()\n",
                "print(\"Résultat avec Pandas :\")\n",
                "print(country_count_pandas.head(1))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Exercice 11"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Résultat avec Spark :\n",
                        "+---------+------+\n",
                        "|  cuisine| count|\n",
                        "+---------+------+\n",
                        "|Mid-range|322000|\n",
                        "+---------+------+\n",
                        "only showing top 1 row\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "cuisine_df = df_parquet.select(explode(split(col(\"cuisines\"), \",\")).alias(\"cuisine\"))\n",
                "cuisine_df = cuisine_df.select(trim(col(\"cuisine\")).alias(\"cuisine\"))\n",
                "cuisine_df = cuisine_df.filter(col(\"cuisine\") != \"\")\n",
                "cuisine_count_spark = cuisine_df.groupBy(\"cuisine\").count().orderBy(col(\"count\").desc())\n",
                "\n",
                "print(\"Résultat avec Spark :\")\n",
                "cuisine_count_spark.show(1)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
